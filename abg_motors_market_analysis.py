# -*- coding: utf-8 -*-
"""ABG Motors Market Analysis

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rWTVVvHiY-WaWD_0mh_eiyvh0BscO_1x
"""

import pandas as pd

df_indian = pd.read_csv("/content/drive/MyDrive/DataSets/IN_Data.xlsx - IN_Mobiles.csv")
df_japanese = pd.read_csv("/content/drive/MyDrive/DataSets/JPN Data.xlsx - CN_Mobiles.csv")

print("Indian dataset loaded successfully.")
print("Japanese dataset loaded successfully.")

print("Indian Dataset Info:")
print(df_indian.info())
print("\nIndian Dataset Head:")
print(df_indian.head())
print("\nIndian Dataset Description:")
print(df_indian.describe())

print("\nJapanese Dataset Info:")
print(df_japanese.info())
print("\nJapanese Dataset Head:")
print(df_japanese.head())
print("\nJapanese Dataset Description:")
print(df_japanese.describe())

"""Initial Cleaning"""

# For Indian Dataset
df_indian['ANN_INCOME'] = df_indian['ANN_INCOME'].str.replace(',', '').astype(int)

# For Japanese Dataset
df_japanese['ANN_INCOME'] = df_japanese['ANN_INCOME'].str.replace(',', '').astype(int)

print("ANN_INCOME cleaned for both datasets.")
print(df_indian['ANN_INCOME'].dtype)
print(df_japanese['ANN_INCOME'].dtype)

import pandas as pd

# Convert DT_MAINT to datetime objects
df_indian['DT_MAINT'] = pd.to_datetime(df_indian['DT_MAINT'])

# Set the reference date (July 1, 2019)
reference_date = pd.to_datetime('2019-07-01')

# Calculate the difference in days
df_indian['DAYS_SINCE_MAINT'] = (reference_date - df_indian['DT_MAINT']).dt.days

# Now you can drop the original DT_MAINT column if you no longer need it
df_indian = df_indian.drop('DT_MAINT', axis=1)

print(df_indian.head())  # Check the new column
print(df_indian.info()) # Check info of the new column

# Bin DAYS_SINCE_MAINT (Indian Dataset) - Choosing appropriate bins!
bins_days = [0, 200, 360, 500, float('inf')]
labels_days = [1, 2, 3, 4]  # Example labels
df_indian['DAYS_SINCE_MAINT_BINNED'] = pd.cut(df_indian['DAYS_SINCE_MAINT'], bins=bins_days, labels=labels_days, right=False)
df_indian = df_indian.drop('DAYS_SINCE_MAINT', axis=1)

# One-hot encode GENDER (both datasets)
df_indian = pd.get_dummies(df_indian, columns=['GENDER'], drop_first=True)
df_japanese = pd.get_dummies(df_japanese, columns=['GENDER'], drop_first=True)

# Japanese Dataset: Bin AGE_CAR (as before)
bins_age = [0, 200, 360, 500, float('inf')]
labels_age = [1, 2, 3, 4]
df_japanese['AGE_CAR_BINNED'] = pd.cut(df_japanese['AGE_CAR'], bins=bins_age, labels=labels_age, right=False)
df_japanese = df_japanese.drop('AGE_CAR', axis=1)

print("Indian Dataset:")
print(df_indian.head())
print(df_indian.info())

print("\nJapanese Dataset:")
print(df_japanese.head())
print(df_japanese.info())

"""EDA"""

import matplotlib.pyplot as plt
import seaborn as sns

# 1. Correlation Matrix (Numerical Features)
correlation_matrix = df_japanese[['CURR_AGE', 'ANN_INCOME']].corr()
print("Correlation Matrix:")
print(correlation_matrix)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# 2. Skewness (Numerical Features)
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df_japanese['CURR_AGE'], kde=True)
plt.title('Distribution of CURR_AGE in Japan')
plt.subplot(1, 2, 2)
sns.histplot(df_japanese['ANN_INCOME'], kde=True)  # Check if the data is skewed
plt.title('Distribution of ANN_INCOME Japan')
plt.show()

# 3. Categorical Feature Exploration
categorical_features = ['AGE_CAR_BINNED', 'GENDER_M']
for feature in categorical_features:
    plt.figure()
    sns.countplot(x=feature, data=df_japanese)
    plt.title(f'Distribution of {feature}')
    plt.show()

# 4. Target Variable Distribution (Japanese Dataset)
plt.figure()
sns.countplot(x='PURCHASE', data=df_japanese)
plt.title('Distribution of PURCHASE')
plt.show()

#Repeat the above steps for Indian dataset as well.

# 1. Correlation Matrix (Numerical Features)
correlation_matrix = df_indian[['CURR_AGE', 'ANN_INCOME']].corr()
print("Correlation Matrix:")
print(correlation_matrix)
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# 2. Skewness (Numerical Features)
plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df_indian['CURR_AGE'], kde=True)
plt.title('Distribution of CURR_AGE')
plt.subplot(1, 2, 2)
sns.histplot(df_indian['ANN_INCOME'], kde=True)  # Check if the data is skewed
plt.title('Distribution of ANN_INCOME')
plt.show()

# 3. Categorical Feature Exploration
categorical_features = ['DAYS_SINCE_MAINT_BINNED', 'GENDER_M']
for feature in categorical_features:
    plt.figure()
    sns.countplot(x=feature, data=df_indian)
    plt.title(f'Distribution of {feature}')
    plt.show()

"""Annual Income log transformation"""

import numpy as np
import pandas as pd

# Apply log transformation to ANN_INCOME (both datasets)
df_indian['ANN_INCOME'] = np.log1p(df_indian['ANN_INCOME'])  # log1p handles 0 values
df_japanese['ANN_INCOME'] = np.log1p(df_japanese['ANN_INCOME'])

print("ANN_INCOME transformed (log1p) for both datasets.")
print("Indian ANN_INCOME:")
print(df_indian['ANN_INCOME'].head())
print("\nJapanese ANN_INCOME:")
print(df_japanese['ANN_INCOME'].head())

# Check the distributions after the transformation
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 6))
plt.subplot(1, 2, 1)
sns.histplot(df_indian['ANN_INCOME'], kde=True)  # Check if the data is skewed
plt.title('Distribution of ANN_INCOME (Indian)')
plt.subplot(1, 2, 2)
sns.histplot(df_japanese['ANN_INCOME'], kde=True)  # Check if the data is skewed
plt.title('Distribution of ANN_INCOME (Japanese)')
plt.show()

"""Model Training
1. Logistics Regression
"""

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

# Prepare the data for modeling (Japanese dataset)
X = df_japanese.drop(['ID', 'PURCHASE'], axis=1)  # Features (excluding ID and target)
y = df_japanese['PURCHASE']  # Target variable

# One-hot encode the categorical features
X = pd.get_dummies(X, columns=['AGE_CAR_BINNED'])

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # 80% train, 20% test

# Train the Logistic Regression model
model = LogisticRegression(max_iter=1000)  # You can adjust hyperparameters here later
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

"""2. Random Forest"""

from sklearn.ensemble import RandomForestClassifier

# Prepare the data for modeling (Japanese dataset) - Same as before
X = df_japanese.drop(['ID', 'PURCHASE'], axis=1)
y = df_japanese['PURCHASE']
X = pd.get_dummies(X, columns=['AGE_CAR_BINNED'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the Random Forest model
rf_model = RandomForestClassifier(random_state=42)  # You can adjust hyperparameters here later
rf_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {accuracy_rf}")
print(classification_report(y_test, y_pred_rf))

# Confusion Matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Random Forest Confusion Matrix')
plt.show()

"""3. Gradient Boosting XGBoost"""

import xgboost as xgb
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

# Prepare the data for modeling (Japanese dataset) - Same as before
X = df_japanese.drop(['ID', 'PURCHASE'], axis=1)
y = df_japanese['PURCHASE']
X = pd.get_dummies(X, columns=['AGE_CAR_BINNED'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Train the XGBoost model
xgb_model = xgb.XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss') #added eval_metric to avoid warning.
xgb_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred_xgb = xgb_model.predict(X_test)

# Evaluate the model
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"XGBoost Accuracy: {accuracy_xgb}")
print(classification_report(y_test, y_pred_xgb))

# Confusion Matrix
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('XGBoost Confusion Matrix')
plt.show()